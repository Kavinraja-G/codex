{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc4b Hola amigo!","text":"<p>I\u2019m a Cloud Enthusiast and DevOps Advocate passionate about building resilient cloud ecosystems and crafting custom Kubernetes solutions. I thrive on solving complex cloud-native challenges and transforming them into secure, cost-efficient, and scalable systems. My journey is driven by continuous learning, mentoring, and an unwavering pursuit of innovation \ud83d\ude80</p> <p>Currently working as a Senior Site Reliability Engineer at a Confidential Identity Company. Formerly with Presidio</p>"},{"location":"blog/","title":"Blogs","text":""},{"location":"blog/#blogs","title":"Blogs","text":"<p>Everything I learn &amp; tinker with </p>"},{"location":"blog/2025/02/22/monitoring-rpi-using-grafana-cloud--alloy/","title":"Monitoring RPi using Grafana Cloud & Alloy","text":""},{"location":"blog/2025/02/22/monitoring-rpi-using-grafana-cloud--alloy/#monitoring-rpi-using-grafana-cloud-alloy","title":"Monitoring RPi using Grafana Cloud &amp; Alloy","text":""},{"location":"blog/2025/02/22/monitoring-rpi-using-grafana-cloud--alloy/#problem","title":"Problem","text":"<p>I have an old Raspberry PI 3 that I used during my college days for applications like drones and robotics. Recently, I decided to repurpose it to run a simple Pi-hole setup to block ads across my home network and devices. One day, my RPi unexpectedly froze, and I had no idea that my DNS requests were failing until I noticed connectivity issues. A reboot resolved the problem, but it made me realize the need for proactive monitoring. Since my router is configured to use Pi-hole as both the primary and secondary DNS (yes, I know I should have a better failover strategy but for now, it works fine), a failure like this can disrupt my entire network.</p> <p>This got me thinking \u2014 why not monitor my Raspberry Pi to catch these issues before they cause problems? Given that it's still a critical piece of hardware in my setup, I need a reliable and highly available monitoring solution. However, I don\u2019t want to self-host my monitoring services; I prefer a managed, cloud-hosted solution that I can trust. Well, I might change this thought in future. More on that later :)</p>"},{"location":"blog/2025/02/22/monitoring-rpi-using-grafana-cloud--alloy/#why-grafana-cloud-alloy","title":"Why Grafana Cloud &amp; Alloy?","text":"<p>Grafana Cloud: I had very good experience with Grafana, Prometheus and its stack in my work, so I thought why not give a try with their Grafana Cloud variant. And then came to know it offers really good free-tier experience which fits for my use-case.</p> <p>What comes with Cloud free?</p> <p>Metrics 10k metrics billable series; Logs, Traces, Profiles 50 GB each all with 14 days retention</p> <p>This is more than enough to monitor my devices.</p> <p>Grafana Alloy: It is a flexible, high-performance, vendor-neutral distribution of the OpenTelemetry Collector which is an alternative for the Grafana agent. It\u2019s fully compatible with popular open-source observability standards like OpenTelemetry and Prometheus. This was my first time using Alloy and I must say, the setup was straightforward and hassle-free.</p> <p>In my setup, Alloy runs as an agent on my Raspberry Pi, collecting and forwarding metrics to Grafana Cloud.</p>"},{"location":"blog/2025/02/22/monitoring-rpi-using-grafana-cloud--alloy/#setup-raspberry-pi-integration","title":"Setup Raspberry Pi Integration","text":"<p>For the actual integration in Grafana Cloud, please follow the official documentation.</p> <p>tldr:</p> <ol> <li>In your Grafana Cloud stack, click <code>Connections</code> in the left-hand menu.</li> <li>Find <code>Raspberry Pi</code> and click its tile to open the integration.</li> <li>Review the prerequisites in the <code>Configuration Details</code> tab and set up Grafana Agent to send Raspberry Pi metrics and logs to your Grafana Cloud instance. (Note: Check next section on the detailed setup.)</li> <li>Click <code>Install</code> to add this integration\u2019s pre-built dashboards and alerts to your Grafana Cloud instance, and you can start monitoring your Raspberry Pi setup.</li> </ol>"},{"location":"blog/2025/02/22/monitoring-rpi-using-grafana-cloud--alloy/#setup-alloy-in-docker","title":"Setup Alloy in Docker","text":"<p>I've installed Docker on my Raspberry Pi, which runs the Alloy agent along with a few other services. I created a Dockerfile that copies all the necessary configurations for Alloy and pushes the image to my private GitHub Registry.</p> <p>(Goodbye, Docker Hub! I'm tired of dealing with rate limits on the free tier...)</p>"},{"location":"blog/2025/02/22/monitoring-rpi-using-grafana-cloud--alloy/#configuring-alloy","title":"Configuring Alloy","text":"<p>Pull the required alloy config from the Raspberry PI integration in Grafana Cloud. For example, here is my config: </p>config.alloy<pre><code>remotecfg {\n    url            = \"https://&lt;YOUR_GRAFANA_REMOTE_FLEET_ENDPOINT&gt;\"\n    id             = \"kavinrpi\"\n    poll_frequency = \"60s\"\n\n    basic_auth {\n        username = \"\"*******\"\n        password = \"*******\"\n    }\n}\n\nprometheus.exporter.unix \"integrations_node_exporter\" { }\n\ndiscovery.relabel \"integrations_node_exporter\" {\n    targets = prometheus.exporter.unix.integrations_node_exporter.targets\n\n    rule {\n        target_label = \"instance\"\n        replacement  = \"kavinrpi\"\n    }\n\n    rule {\n        target_label = \"job\"\n        replacement  = \"integrations/raspberrypi-node\"\n    }\n}\n\nprometheus.scrape \"integrations_node_exporter\" {\n    targets    = discovery.relabel.integrations_node_exporter.output\n    forward_to = [prometheus.relabel.integrations_node_exporter.receiver]\n    job_name   = \"integrations/node_exporter\"\n}\n\n// Very minimal metrics which I export, you could add others as well\nprometheus.relabel \"integrations_node_exporter\" {\n    forward_to = [prometheus.remote_write.metrics_service.receiver]\n\n    rule {\n        source_labels = [\"__name__\"]\n        regex         = \"up|node_boot_time_seconds|node_cpu_seconds_total|node_disk_io_time_seconds_total|node_disk_io_time_weighted_seconds_total|node_disk_read_bytes_total|node_disk_written_bytes_total|node_filesystem_avail_bytes|node_filesystem_files|node_filesystem_files_free|node_filesystem_readonly|node_filesystem_size_bytes|node_hwmon_temp_celsius|node_load1|node_load15|node_load5|node_memory_Buffers_bytes|node_memory_Cached_bytes|node_memory_MemAvailable_bytes|node_memory_MemFree_bytes|node_memory_MemTotal_bytes|node_memory_Slab_bytes|node_memory_SwapTotal_bytes|node_network_receive_bytes_total|node_network_receive_drop_total|node_network_receive_errs_total|node_network_receive_packets_total|node_network_transmit_bytes_total|node_network_transmit_drop_total|node_network_transmit_errs_total|node_network_transmit_packets_total|node_os_info|node_systemd_unit_state|node_uname_info|node_vmstat_pgmajfault\"\n        action        = \"keep\"\n    }\n}\n\n// Write metrics to your Grafana Cloud Prometheus instance\nprometheus.remote_write \"metrics_service\" {\n    endpoint {\n        url = \"https://&lt;YOUR_PROMETHEUS_ENDPOINT&gt;/api/prom/push\"\n\n        basic_auth {\n            username = \"*******\"\n            password = \"*******\"\n        }\n    }\n}\n</code></pre><p></p> <p>All the above endpoints will be available once you complete the steps in the Raspberry Pi integration window in Grafana Cloud.</p> <p>To enable log collection, append the following configuration to your <code>config.alloy</code> </p><pre><code>discovery.relabel \"logs_integrations_integrations_node_exporter_journal_scrape\" {\n    targets = []\n\n    rule {\n        source_labels = [\"__journal__systemd_unit\"]\n        target_label  = \"unit\"\n    }\n\n    rule {\n        source_labels = [\"__journal__boot_id\"]\n        target_label  = \"boot_id\"\n    }\n\n    rule {\n        source_labels = [\"__journal__transport\"]\n        target_label  = \"transport\"\n    }\n\n    rule {\n        source_labels = [\"__journal_priority_keyword\"]\n        target_label  = \"level\"\n    }\n}\n\nloki.source.journal \"logs_integrations_integrations_node_exporter_journal_scrape\" {\n    max_age       = \"24h0m0s\"\n    relabel_rules = discovery.relabel.logs_integrations_integrations_node_exporter_journal_scrape.rules\n    forward_to    = [loki.write.grafana_cloud_loki.receiver]\n    labels        = {\n        instance = constants.hostname,\n        job      = \"integrations/raspberrypi-node\",\n    }\n}\n\nlocal.file_match \"logs_integrations_integrations_node_exporter_direct_scrape\" {\n    path_targets = [{\n        __address__ = \"localhost\",\n        __path__    = \"/var/log/{syslog,messages,*.log}\",\n        instance    = constants.hostname,\n        job         = \"integrations/raspberrypi-node\",\n    }]\n}\n\nloki.source.file \"logs_integrations_integrations_node_exporter_direct_scrape\" {\n    targets    = local.file_match.logs_integrations_integrations_node_exporter_direct_scrape.targets\n    forward_to = [loki.write.grafana_cloud_loki.receiver]\n}\n</code></pre><p></p>"},{"location":"blog/2025/02/22/monitoring-rpi-using-grafana-cloud--alloy/#creating-dockerfile","title":"Creating Dockerfile","text":"Dockerfile<pre><code>FROM grafana/alloy:v1.3.1\n\n# Copy the custom config file into the image\nCOPY ./configs/rpi-config.alloy /etc/alloy/config.alloy\n\nCMD [\"run\", \"--server.http.listen-addr=0.0.0.0:12345\", \"--stability.level=public-preview\", \"/etc/alloy/config.alloy\"]\n</code></pre> <p>Now, you can build and push the images to your registry. For example, I use Podman as my container management tool </p>Build &amp; Push Images<pre><code>## Build &amp; Push using podman with multi-arch images\npodman manifest create rpi-alloy\npodman build -f rpi.Dockerfile --platform linux/amd64,linux/arm64 -t &lt;REGISTRY&gt;/&lt;USERNAME&gt;/alloy:rpi-0.1.0 --manifest localhost/rpi-alloy\npodman manifest push localhost/rpi-alloy &lt;REGISTRY&gt;/&lt;USERNAME&gt;/alloy:rpi-0.1.0\n</code></pre><p></p>"},{"location":"blog/2025/02/22/monitoring-rpi-using-grafana-cloud--alloy/#fleet-management","title":"Fleet Management","text":"<p>Once the agent is up and running on your Raspberry Pi, it will appear in the fleet management dashboard, as shown below. You can add multiple agents depending on your setup and monitoring needs.</p> <p></p>"},{"location":"blog/2025/02/22/monitoring-rpi-using-grafana-cloud--alloy/#dashboards-alerting","title":"Dashboards &amp; Alerting","text":"<p>By default, when you set up the integration, Grafana automatically installs dashboards for both logs and metrics. Alert recording rules comes by default as well for these dashboards, you can cusomtize the alerting as you wish. Here\u2019s how they look:</p> <p>Dashboard:</p> <p></p> <p>Alert Rules:</p> <p></p> <p>Under Alerts &amp; IRM -&gt; Alerting -&gt; Contact Points add your default email contact for notifications.</p> <p></p>"},{"location":"blog/2025/02/22/monitoring-rpi-using-grafana-cloud--alloy/#final-points","title":"Final points","text":"<p>Monitoring my Raspberry Pi with Grafana Alloy and Grafana Cloud proved to be an easy and effective solution. Alloy collects system metrics and logs, while Grafana Cloud provides a reliable, hassle-free observability platform. With this setup, I have real-time insights into my Pi. In the future, I may explore Loki for enhanced logging and setup monitoring PiHole as well. How do you monitor your homelab? \ud83d\ude80</p>"},{"location":"projects/crossplane-docs/","title":"Crossplane Docs","text":""},{"location":"projects/crossplane-docs/#what-is-xdocs","title":"What is XDocs?","text":"<ul> <li>We have XR, XRD, XRC ;) Why not XDocs?</li> <li>Inspired from terraform-docs but for Crossplane</li> <li>Generate markdown based docs for your Compositions, it also includes your linked Resources, XRD and Claim</li> </ul>"},{"location":"projects/crossplane-docs/#installation","title":"Installation","text":"<p>XDocs Generator can be installed in several ways to suit your preferences and development environments</p>"},{"location":"projects/crossplane-docs/#homebrew-tap-macos-linux","title":"Homebrew Tap (macOS &amp; Linux)","text":"<p>If you're using Homebrew, you can install the tool via our custom Homebrew tap</p> <pre><code>brew tap Kavinraja-G/tap\nbrew install crossplane-docs\n</code></pre>"},{"location":"projects/crossplane-docs/#standalone-binary","title":"Standalone Binary","text":"<p>For macOS, Linux, and Windows, standalone binaries are available. Download the appropriate binary for your operating system from the releases page, then move it to a directory in your PATH.</p>"},{"location":"projects/crossplane-docs/#macoslinux","title":"macOS/Linux","text":"<pre><code># Feel free to change the release/arch names accordingly\ncurl -Lo crossplane-docs https://github.com/Kavinraja-G/crossplane-docs/releases/download/v0.1.2/crossplane-docs_v0.1.2_darwin_amd64.tar.gz\nchmod +x crossplane-docs\nsudo mv crossplane-docs /usr/local/bin/\n</code></pre>"},{"location":"projects/crossplane-docs/#windows","title":"Windows","text":"<p>Download the <code>.exe</code> file and add it to your <code>PATH</code></p>"},{"location":"projects/crossplane-docs/#usage","title":"Usage","text":"<p>Currently xDocs supports only markdown output, but more in pipeline. To generate markdown docs for your compositions &amp; XRDs </p><pre><code>crossplane-docs md [INPUT_PATH|INPUT_FILE] -o [OUTPUT_FILE]\n</code></pre> For example: <pre><code>crossplane-docs md ./samples -o samples/README.md\n</code></pre> Check README.md for the output.<p></p>"},{"location":"projects/crossplane-docs/#features","title":"Features","text":"<ul> <li>Discovers Composition &amp; its resources, XR, XRD and Claim names with their references</li> <li>Markdown ouput with tabulation</li> <li>Claim/XRC specifications (WIP)</li> </ul>"},{"location":"projects/crossplane-docs/#limitations","title":"Limitations","text":"<p>Crossplane compositions with Pipeline composition mode is not supported.</p>"},{"location":"projects/nodegizmo/","title":"Nodegizmo","text":"<p>A small command-line utility for your Kubernetes nodes.</p>"},{"location":"projects/nodegizmo/#installation","title":"Installation","text":"<p><code>nodegizmo</code> kubectl plugin is available in krew plugin manager. Anyone can install with the following steps:</p> <ul> <li>Install <code>krew</code> for kuebctl using the following doc.</li> <li>Run <code>kubectl krew install nodegizmo</code></li> </ul>"},{"location":"projects/nodegizmo/#features","title":"Features","text":""},{"location":"projects/nodegizmo/#nodegizmo-node","title":"nodegizmo node","text":"<p>This command displays the generic node related information. For example:</p> <ul> <li>NodeName</li> <li>K8sVersion</li> <li>Image</li> <li>OS &amp; Architecture info</li> <li>NodeStatus (Ready/NotReady)</li> <li>Taints</li> <li>Node Provider (AWS/Azure/GCP)</li> <li>Topology info (Region &amp; Zone)</li> </ul> <p></p>"},{"location":"projects/nodegizmo/#nodegizmo-node-capacity","title":"nodegizmo node capacity","text":"<p>Node Capacity information</p> <ul> <li>CPU</li> <li>Memory</li> <li>Disk</li> <li>Ephemeral storage</li> <li>Pod capacities</li> <li>Nodepool related information</li> </ul> <p></p>"},{"location":"projects/nodegizmo/#nodegizmo-nodepool","title":"nodegizmo nodepool","text":"<p>Nodepool related information</p> <ul> <li>Grouped by NodePool ID</li> <li>Node list</li> <li>Topology info (Region &amp; Zone)</li> <li>Instance type</li> <li>K8sVersion</li> <li>Nodepool provider (supported: EKS/AKS/GKE/Karpenter)</li> </ul> <p></p>"},{"location":"projects/nodegizmo/#nodegizmo-exec-nodename","title":"nodegizmo exec nodeName","text":"<p>Exec into any node by spawning a <code>nsenter</code> pod automatically based on the node selection.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2025/#2025","title":"2025","text":""},{"location":"blog/category/homelab/","title":"HomeLab","text":""},{"location":"blog/category/homelab/#homelab","title":"HomeLab","text":""},{"location":"blog/category/monitoring/","title":"Monitoring","text":""},{"location":"blog/category/monitoring/#monitoring","title":"Monitoring","text":""}]}